{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Brief: Data Streaming with Kafka\n",
        "\n",
        "## Background: Telecommunications Mobile Money Data Engineering with\n",
        "Kafka\n",
        "\n",
        "In this project, you will work with telecommunications mobile money data to build a Kafka data engineering solution. You will be provided with a dummy json file containing sample data that you will use to test your solution.\n",
        "\n",
        "The project aims to build a Kafka pipeline that can receive real-time data from\n",
        "telecommunications mobile money transactions and process it for analysis. The pipeline should be designed to handle high volumes of data and ensure that the data is processed efficiently.\n",
        "\n",
        "\n",
        "To complete this project, you will need to follow these steps:\n",
        "\n",
        "1. Set up a Kafka cluster: You must set up a Kafka cluster that can handle high volumes\n",
        "of data. You can use either a cloud-based or on-premises Kafka cluster.\n",
        "2. Develop a Kafka producer: You must develop a Kafka producer that can ingest data\n",
        "from telecommunications mobile money transactions and send it to the Kafka cluster.\n",
        "The producer should be designed to handle high volumes of data and ensure that the\n",
        "data is sent to the Kafka cluster efficiently.\n",
        "3. Develop a Kafka consumer: You must develop a Kafka consumer to receive data from\n",
        "the Kafka cluster and process it for analysis. The consumer should be designed to\n",
        "handle high volumes of data and ensure that the data is processed efficiently.\n",
        "4. Process the data: Once you have set up the Kafka pipeline, you must process the data\n",
        "for analysis. This may involve cleaning and aggregating the data, performing\n",
        "calculations, and creating visualizations.\n",
        "5. Test the solution: You must test your solution using the provided dummy json file.\n",
        "\n",
        "\n",
        "Thefile contains sample data that you can use to ensure that your Kafka pipeline is working\n",
        "correctly."
      ],
      "metadata": {
        "id": "qrqSnxj6Wo99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93MihUvyH-_W"
      },
      "outputs": [],
      "source": [
        "#Install confluent library\n",
        "!pip install confluent-kafka\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import needed libraries\n",
        "from confluent_kafka import Producer, Consumer\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from collections import OrderedDict\n",
        "import random\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "\n",
        "import logging\n",
        "#Setup logger\n",
        "logging.basicConfig(filename='pipeline.log', level=logging.DEBUG)\n",
        "\n",
        "#Using confluent cloud to host kafka instance\n",
        "# Confluent Cloud configurations\n",
        "\n",
        "bootstrap_servers = '<server_name>'\n",
        "security_protocol = 'SASL_SSL'\n",
        "sasl_mechanism = 'PLAIN'\n",
        "sasl_plain_username = '<username>'\n",
        "sasl_plain_password = '<password>'\n",
        "topic = 'my_pipeline'"
      ],
      "metadata": {
        "id": "OOWRJagFXKyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Producer configuration\n",
        "producer_conf = {\n",
        "    'bootstrap.servers': bootstrap_servers,\n",
        "    'security.protocol': security_protocol,\n",
        "    'sasl.mechanism': sasl_mechanism,\n",
        "    'sasl.username': sasl_plain_username,\n",
        "    'sasl.password': sasl_plain_password\n",
        "}"
      ],
      "metadata": {
        "id": "0qVbWvYcXQQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consumer configuration\n",
        "consumer_conf = {\n",
        "    'bootstrap.servers': bootstrap_servers,\n",
        "    'security.protocol': security_protocol,\n",
        "    'sasl.mechanism': sasl_mechanism,\n",
        "    'sasl.username': sasl_plain_username,\n",
        "    'sasl.password': sasl_plain_password,\n",
        "    'group.id': 'my_consumer_group'\n",
        "}"
      ],
      "metadata": {
        "id": "Ei9WQlEqXX7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper functions for the cli output table art\n",
        "\n",
        "def border1(text):\n",
        "\n",
        "  \"\"\"\n",
        "    Function used to draw the grid outline on cli output using tabulate library\n",
        "  \"\"\"\n",
        "  data_store = [[text]]\n",
        "  output = tabulate(data_store, tablefmt='grid')\n",
        "  print(output)\n",
        "\n",
        "  def border2(text):\n",
        "\n",
        "  \"\"\"\n",
        "    Function used to draw the fancy_grid outline upon loop exit using tabulate library\n",
        "  \"\"\"\n",
        "  data_store = [[text]]\n",
        "  output = tabulate(data_store, tablefmt='fancy_grid')\n",
        "  print(output)"
      ],
      "metadata": {
        "id": "V17HO1PLXar1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Producer config\n",
        "\n",
        "# Create the Kafka producer\n",
        "producer = Producer(producer_conf)\n",
        "\n",
        "\n",
        "def json_generator():\n",
        "\n",
        "  \"\"\"\n",
        "  Function generates random customer data for producer to load to topic hosted on confluent cloud\n",
        "  \"\"\"\n",
        "  x=0   #a loop counter\n",
        "  dict_list = []\n",
        "\n",
        "  try:\n",
        "    while x<5:\n",
        "        # Generate random transaction ID\n",
        "        transaction_id = str(random.randint(10000, 99999))\n",
        "\n",
        "        # Generate random phone numbers\n",
        "        sender_phone_number = \"256\" + \"\".join(str(random.randint(0, 9)) for _ in range(9))\n",
        "        receiver_phone_number = \"256\" + \"\".join(str(random.randint(0, 9)) for _ in range(9))\n",
        "\n",
        "        # Generate random transaction amount\n",
        "        transaction_amount = random.randint(1, 100000)\n",
        "\n",
        "        # Generate random transaction time\n",
        "        transaction_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        # Create the JSON object\n",
        "        data = {\n",
        "            \"transaction_id\": transaction_id,\n",
        "            \"sender_phone_number\": sender_phone_number,\n",
        "            \"receiver_phone_number\": receiver_phone_number,\n",
        "            \"transaction_amount\": transaction_amount,\n",
        "            \"transaction_time\": transaction_time\n",
        "        }\n",
        "\n",
        "        dict_list.append(data)\n",
        "        x=x+1\n",
        "\n",
        "        #sleep 1 second so that timestamp values vary\n",
        "        time.sleep(1)"
      ],
      "metadata": {
        "id": "1nIuLsUiXeAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Produce sample CDR data to Kafka topic\n",
        "    for trx in dict_list:\n",
        "      # Serialize transaction data to JSON\n",
        "      serialized_trx = json.dumps(trx).encode('utf-8')\n",
        "      # Produce message to Kafka topic\n",
        "      producer.produce(topic, key=None, value=serialized_trx)\n",
        "      producer.flush()\n",
        "\n",
        "    print('Producer posted sample transaction data to Kafka topic....')\n",
        "\n",
        "  except Exception as e:\n",
        "    err = \"Producer() error - \"+str(e)\n",
        "    logging.debug(err)"
      ],
      "metadata": {
        "id": "snqUsHvOXnhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Kafka consumer\n",
        "consumer = Consumer(consumer_conf)\n",
        "\n",
        "# Subscribe to the topic\n",
        "consumer.subscribe([topic])\n",
        "\n",
        "#The _main-pipeline_ function\n",
        "\n",
        "def main_pipeline():\n",
        "\n",
        "  \"\"\"\n",
        "  Main pipeline function that starts by producing and posting to the topic | It then enters a while loop where consumer reads messages from topic, this message data is used to analyse customer transaction spend/\n",
        "  unique sender/unique recipient/ max spend/ min spend. Running stats are posted in realtime as each message is consumed\n",
        "  \"\"\"\n",
        "\n",
        "  # Variables to store data\n",
        "  transaction_count = 0\n",
        "  total_transaction_amount = 0\n",
        "  transaction_amount_histogram = defaultdict(int)\n",
        "  trx_amount_tracker = {}\n",
        "  unique_sender_numbers = set()\n",
        "  unique_receiver_numbers = set()\n",
        "\n",
        "  #produce some data first...\n",
        "  json_generator()\n",
        "\n",
        "  try:\n",
        "      print(\"Pipeline 'While' loop started.... Use Stop[on jupyter] or Ctrl+C[in bash] to stop the loop \\n\")\n",
        "      while True:\n",
        "\n",
        "          msg = consumer.poll(1.0)\n",
        "\n",
        "          if msg is None:\n",
        "              json_generator()\n",
        "              continue\n",
        "\n",
        "          if msg.error():\n",
        "              print(\"Consumer error: {}\".format(msg.error()))\n",
        "              continue\n",
        "\n",
        "          # Process the consumed message\n",
        "          message = json.loads(msg.value())\n",
        "          trx_amount_tracker[message.get('transaction_id')] = int(message.get('transaction_amount'))\n",
        "          transaction_amount = int(message.get('transaction_amount'))\n",
        "          sender_phone_number = message.get('sender_phone_number')\n",
        "          receiver_phone_number = message.get('receiver_phone_number')\n",
        "\n",
        "          # Update tracker data\n",
        "          total_transaction_amount += transaction_amount\n",
        "          transaction_count += 1\n",
        "          transaction_amount_histogram[transaction_amount] += 1\n",
        "          unique_sender_numbers.add(sender_phone_number)\n",
        "          unique_receiver_numbers.add(receiver_phone_number)\n",
        "\n",
        "          # Print the processed/consumed message\n",
        "          border1(\"\\nProcessed Message:\"+json.dumps(message, indent=4))\n",
        "          print(\"\")\n",
        "\n",
        "          # Print the aggregated data\n",
        "          print(\"Total of all transactions[Kshs]: \", total_transaction_amount)\n",
        "\n",
        "          print(\"Top 5 transaction amounts table:\\n\")\n",
        "          sorted_items = dict(sorted(trx_amount_tracker.items(),  key=lambda x: x[1], reverse=True)[:5])\n",
        "          for key,value in sorted_items.items():\n",
        "                print(f\"Trx amount [Kshs]: {value}: Trx ID: {key}\")\n",
        "\n",
        "          # Find the largest and smallest trx spend amount values\n",
        "          largest_value = max(trx_amount_tracker.values())\n",
        "          smallest_value = min(trx_amount_tracker.values())\n",
        "          # Find the corresponding keys for min, max...\n",
        "          largest_keys = [key for key, value in trx_amount_tracker.items() if value == largest_value]\n",
        "          smallest_keys = [key for key, value in trx_amount_tracker.items() if value == smallest_value]\n",
        "\n",
        "          # Print the largest and smallest keys and values\n",
        "          for trx in largest_keys:\n",
        "            print(f\"\\nLargest trx amounts. Trx ID {trx} - Amount {trx_amount_tracker[trx]}\")\n",
        "\n",
        "          for trx in smallest_keys:\n",
        "            print(f\"Smallest trx amounts. Trx ID {trx} - Amount {trx_amount_tracker[trx]} \\n\")\n",
        "\n",
        "\n",
        "          print(\"Number of Unique Sender Phone Numbers:\", len(unique_sender_numbers))\n",
        "          print(\"Number of Unique Receiver Phone Numbers:\", len(unique_receiver_numbers))\n",
        "\n",
        "\n",
        "  except KeyboardInterrupt:\n",
        "      pass\n",
        "\n",
        "  finally:\n",
        "      border2(\"Loop exit - Finally...closing the consumer...\")\n",
        "\n",
        "      #close the consumer()\n",
        "  consumer.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run the data pipeline function\n",
        "    main_pipeline()"
      ],
      "metadata": {
        "id": "H-UThGKOXqE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}